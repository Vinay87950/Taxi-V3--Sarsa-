import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import imageio
import os

np.random.seed(42)

"""
Goal for taxi env - the rewards generated by this method are negative, 
because the reward strucutre is such that the reward generated for every step is -1, and we must minimize this.

So we should consider the lambda value such that the least negative average reward is generated.

So my objective is to find the lambda value that minimizes these negative rewards - in other words, 
the lambda that helps the agent complete its task in the fewest steps while avoiding mistakes
"""


'''
Selecting an action using epsilon greedy policy
'''

def epsilon_greedy(q_values, epsilon, n_actions):
    # explore choose a random action
    if np.random.random() < epsilon:
        return np.random.randint(n_actions)
    else:
        # exploit choose the best action
        return np.argmax(q_values)
    

def sarsa(env, _lambda, num_episodes=3000, alpha=0.1, gamma=0.99, epsilon=0.1, record_last=False):
    """
    Implements the SARSA(λ) algorithm with optional video recording.
    """
    state_space = env.observation_space.n
    action_space = env.action_space.n
    
    q_values = np.zeros((state_space, action_space))
    episode_reward = []
    

    # For recording the last episode
    frames = []
    
    # iterate over the episodes
    for episode in range(num_episodes):

        # Set eligibility traces at the start of each episode
        eligibility_trace = np.zeros((state_space, action_space))
        
        state = env.reset()[0]  # Get only the state from reset
        done = False
        total_reward = 0
        
        # choose the first action using epsilon greedy policy
        action = epsilon_greedy(q_values[state], epsilon, action_space)
        
        # Record the last episode if requested
        if record_last and episode == num_episodes - 1:
            frames.append(env.render())
        
        # iterate over the steps
        while not done:
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            if reward == 1:
                reward = 20
            elif terminated and reward == 0:
                reward = -5
            
            # Record frame if it's the last episode
            if record_last and episode == num_episodes - 1:
                frames.append(env.render())
            
            # choose the next action using epsilon greedy policy
            next_action = epsilon_greedy(q_values[next_state], epsilon, env.action_space.n) if not done else 0
            
            # Calculate TD error
            if done:
                td_error = reward - q_values[state, action]
            else:
                td_error = reward + gamma * q_values[next_state, next_action] - q_values[state, action]
            

            # Decay eligibility traces
            eligibility_trace *= gamma * _lambda
            
            # Update eligibility trace for current state-action pair
            eligibility_trace[state, action] += 1.0
            
            # Update all state-action pairs according to their eligibility
            q_values += alpha * td_error * eligibility_trace
            
            total_reward += reward
            
            if done:
                break
            
            state = next_state
            action = next_action
        
        episode_reward.append(total_reward)
        
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_reward[-100:])
            success_rate = (np.array(episode_reward[-100:]) > 0).mean() * 100
            print(f"Lambda {_lambda}, Episode {episode + 1}/{num_episodes}: "
                  f"Average Reward: {avg_reward:.2f}, Success Rate: {success_rate:.2f}%")
    
    if record_last:
        os.makedirs('video', exist_ok=True)
        imageio.mimsave(f'video/lambda_{_lambda}.gif', frames)
    
    return episode_reward, q_values


# Train and compare different lambda values
env = gym.make('Taxi-v3', render_mode='rgb_array')
lambda_values = [0, 0.7 ,0.8, 0.9, 1.0]
all_rewards = {}

# Train for each lambda value
for lambda_val in lambda_values:
    print(f"\nTraining with lambda = {lambda_val}")
    rewards, _ = sarsa(env, lambda_val, record_last=True)
    all_rewards[lambda_val] = rewards


# Plotting the results
plt.figure(figsize=(12, 6))
window_size = 100

for lambda_val, rewards in all_rewards.items():
    # Smoothing the rewards
    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
    plt.plot(smoothed_rewards, label=f'λ={lambda_val}')

plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title('SARSA(λ)')
plt.legend()
plt.grid(True)
plt.savefig('comparison.png')
plt.close()

Github repo - "https://github.com/aditya1702/Machine-Learning-and-Data-Science/blob/master/Implementation%20of%20Reinforcement%20Learning%20Algorithms/Tensorflow%20Implementations/Eligibility%20Traces/Tabular%20SARSA(%20%CE%BB%20).ipynb"
'''

